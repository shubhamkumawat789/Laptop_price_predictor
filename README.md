ğŸ’» Laptop Price Predictor (Machine Learning)

This project predicts the price of a laptop based on its hardware specifications using machine learning.
You give inputs like brand, RAM, CPU, storage, screen details etc., and the model returns an estimated price.

ğŸ§  Project Idea:

We have a dataset of many laptops with:

Their specifications (brand, RAM, CPU, GPU, storage, screen, etc.)

Their actual selling price

The model learns the relationship between specs and price.

After training, we can give it a new laptopâ€™s specs, and it will guess the price.

ğŸ—‚ï¸ Dataset & Important Columns

The original CSV is laptop_data.csv.
Some important original columns:

Company â€“ Laptop brand (Dell, HP, Apple, etc.)

TypeName â€“ Type of laptop (Gaming, Ultrabook, Notebook, etc.)

Inches â€“ Screen size

ScreenResolution â€“ Resolution + extra info (e.g. â€œ1920x1080 IPSâ€, â€œTouchscreenâ€)

Cpu â€“ Full CPU name (e.g. Intel Core i5 7200U)

Ram â€“ RAM size (e.g. 8GB, 16GB)

Memory â€“ Storage (e.g. â€œ128GB SSD + 1TB HDDâ€)

Gpu â€“ GPU information

OpSys â€“ Operating system (Windows, macOS, Linux, etc.)

Weight â€“ Weight of the laptop

Price â€“ Target variable (what we want to predict)

ğŸ§¹ Data Cleaning & Feature Engineering (what you did to the data)

To make the data suitable for ML, the notebook performs several steps:

1ï¸âƒ£ Handling screen features

From ScreenResolution and Inches you created:

Touchscreen â€“

1 if â€œTouchscreenâ€ is present

0 otherwise

Ips â€“

1 if â€œIPSâ€ is present

0 otherwise

X_res, Y_res â€“ numeric resolution values (e.g. 1920 and 1080)

ppi â€“ Pixels Per Inch
Computed as:

ğ‘
ğ‘
ğ‘–
=
ğ‘‹
_
ğ‘Ÿ
ğ‘’
ğ‘ 
2
+
ğ‘Œ
_
ğ‘Ÿ
ğ‘’
ğ‘ 
2
Inches
ppi=
Inches
X_res
2
+Y_res
2
	â€‹

	â€‹


Then you drop the original columns:

ScreenResolution, Inches, X_res, Y_res

So the model uses Touchscreen, IPS, and PPI instead of raw resolution text.

2ï¸âƒ£ CPU simplification

From Cpu you created:

Cpu Name â€“ first 3 words (e.g. â€œIntel Core i5â€)

Cpu brand â€“ grouped into:

Intel Core i7

Intel Core i5

Intel Core i3

Other Intel Processor

AMD Processor

Then you drop:

Cpu, Cpu Name

This gives a simple categorical feature for CPU power.

3ï¸âƒ£ Memory â†’ HDD / SSD / Hybrid / Flash

Memory is messy (like â€œ128GB SSD + 1TB HDDâ€). You cleaned it step by step:

Remove .0, GB, TB (TB is converted to 000 GB).

Split into two parts (first drive and second drive).

Detect whether each layer is:

HDD

SSD

Hybrid

Flash Storage

Finally create numeric columns:

HDD â€“ total HDD storage (in GB)

SSD â€“ total SSD storage (in GB)

Hybrid â€“ total hybrid storage (in GB)

Flash_Storage â€“ total flash storage (in GB)

Drop helper columns used in the process.

Now the model gets clean numerical storage features.

4ï¸âƒ£ GPU brand

From Gpu:

Extract Gpu brand = first word (e.g. Intel, Nvidia, AMD)

Remove rows where Gpu brand == 'ARM' (rare/unwanted)

Drop the original Gpu column

5ï¸âƒ£ Operating system grouping

From OpSys you define a new column os:

Windows â€“ for Windows 7 / 10 / 10 S

Mac â€“ for macOS / Mac OS X

Others/No OS/Linux â€“ everything else

Then drop OpSys.

6ï¸âƒ£ Final features and target

Features X = all columns except Price

Target y = log of Price

Taking log smooths the distribution and helps the model.

You then split into train & test:

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.15, random_state=2
)

ğŸ¤– Models & Approach

You use scikit-learn Pipelines with a ColumnTransformer:

ColumnTransformer:

One-hot encodes the categorical columns (Company, TypeName, Cpu brand, Gpu brand, os)

Keeps the remaining numeric columns as-is (RAM, Weight, Touchscreen, Ips, ppi, HDD, SSD, Hybrid, Flash_Storage, etc.)

You try multiple regression models:

Linear models: LinearRegression, Ridge, Lasso

KNN: KNeighborsRegressor

Tree model: DecisionTreeRegressor

Ensemble models:

RandomForestRegressor

ExtraTreesRegressor

AdaBoostRegressor

GradientBoostingRegressor

SVM: SVR

Gradient boosting library: XGBRegressor (XGBoost)

For each model, you:

Fit on X_train, y_train

Predict on X_test

Evaluate using:

RÂ² score (how well it explains variance)

MAE (Mean Absolute Error) (average error in log price)

âœ… Final chosen model

From the last cells in the notebook:

Final pipe is a Pipeline with:

Step 1: ColumnTransformer (one-hot encode selected columns)

Step 2: XGBRegressor with tuned parameters (e.g. n_estimators=45, max_depth=5, learning_rate=0.5)

This final pipe is what you export and use for predictions.

ğŸ’¾ Saving the model

At the end of the notebook:

import pickle

pickle.dump(df, open('df.pkl', 'wb'))
pickle.dump(pipe, open('pipe.pkl', 'wb'))


df.pkl â€“ processed dataset

pipe.pkl â€“ full pipeline (preprocessing + XGBoost model)

In your Streamlit app, you will load pipe.pkl and call:

pred_log = pipe.predict(input_df)[0]
pred_price = np.exp(pred_log)  # convert back from log to actual price

ğŸ—ï¸ Project Structure (suggested)

You can organize the repo like:

.
â”œâ”€â”€ app.py                      # Streamlit app
â”œâ”€â”€ laptop-price-predictor.ipynb
â”œâ”€â”€ laptop_data.csv             # Raw data
â”œâ”€â”€ pipe.pkl                    # Trained model pipeline
â”œâ”€â”€ df.pkl                      # Processed dataset
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ How to Run the Project
1ï¸âƒ£ Install dependencies
pip install -r requirements.txt


Typical packages used:

numpy

pandas

matplotlib

seaborn

scikit-learn

xgboost

streamlit

2ï¸âƒ£ (Optional) Retrain the model

If you want to retrain:

Open laptop-price-predictor.ipynb in Jupyter / VS Code.

Run all cells.

It will recreate df.pkl and pipe.pkl.

3ï¸âƒ£ Run the Streamlit app
streamlit run app.py


Then:

A browser window opens.

You select:

Company, TypeName, CPU brand, GPU brand, OS

RAM, storage (HDD/SSD), weight, touchscreen yes/no, IPS yes/no, etc.

Click the Predict button.

The app shows the predicted laptop price.

ğŸ¯ Goal of the Project

Understand how different laptop specs affect price.

Practice data cleaning, feature engineering, and model comparison.

Build a deployable ML model using a Pipeline so:

Preprocessing and model stay together

You can easily load it in a web app (Streamlit) and make predictions.
